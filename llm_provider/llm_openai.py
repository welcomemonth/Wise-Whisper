from openai import OpenAI
import requests

llm = OpenAI(
    # base_url = 'http://localhost:11434/v1',
    # base_url='https://llm.meetmonth.top/v1',
    # base_url='https://api.openai.com/v1',
    base_url='https://macaw-pleasing-jawfish.ngrok-free.app/v1',
    api_key='ollama'  # required, but unused
)


# é€šè¿‡GET /api/tagsè·å–æœ¬åœ°æ¨¡å‹æ¥å£åˆ—è¡¨
def get_model_list():
    base_url = 'https://macaw-pleasing-jawfish.ngrok-free.app/api/tags'
    # å‘é€ GET è¯·æ±‚ä»¥è·å–æ¨¡å‹åˆ—è¡¨
    response = requests.get(base_url)

    # æ£€æŸ¥è¯·æ±‚æ˜¯å¦æˆåŠŸ
    if response.status_code == 200:
        # è§£æå“åº”çš„ JSON æ•°æ®
        models = response.json()['models']
        models_list = [model['name'] for model in models]
        return models_list
    else:
        print(f"æ— æ³•è·å–æ¨¡å‹åˆ—è¡¨ï¼ŒHTTP çŠ¶æ€ç ï¼š{response.status_code}")
        return []


# å¯¹å¤§è¯­è¨€æ¨¡å‹å›å¤æ¥å£è¿›è¡Œå†æ¬¡å°è£…ï¼Œåªéœ€è¦ç”¨æˆ·ä¼ é€’èŠå¤©å†…å®¹
def ask(messages, model='llama2:13b', **kwargs):
    # print(kwargs)
    return llm.chat.completions.create(
        model=model,
        messages=messages,
        **kwargs
    )


# æµ‹è¯•å¤§è¯­è¨€æ¨¡å‹æ˜¯å¦å¯ç”¨ï¼Œä¸å¯ç”¨è¿”å›False
def test_llm_availability():
    try:
        # response = llm.models.list()
        return True
    except Exception as e:
        print(f"æ— æ³•è¿æ¥åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼Œé”™è¯¯ä¿¡æ¯ï¼š{e}")
        return False


if __name__ == "__main__":
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The LA Dodgers won in 2020."},
        {"role": "user", "content": "ç»™æˆ‘è®²ä¸€ä¸ªå†·ç¬‘è¯"}
    ]
    res = ask(messages, stream=False)

    # for index, chunk in enumerate(res):
    #     if index == 0:
    #         print(chunk)
    # ChatCompletionChunk(id='chatcmpl-263', choices=[Choice(delta=ChoiceDelta(content='OK', function_call=None, role='assistant', tool_calls=None), finish_reason=None, index=0, logprobs=None)], created=1719298104, model='llama2:13b', object='chat.completion.chunk', service_tier=None, system_fingerprint='fp_ollama', usage=None)
    #     print(chunk.choices[0].delta.content, end="")
    # )
    print(res)
    # ChatCompletion(id='chatcmpl-376', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content="Okay! Here's a cold one for you:\n\nWhy did the tomato turn down the date with the avocado?\n\nBecause it was a fruit-less relationship! ğŸ˜œ", role='assistant', function_call=None, tool_calls=None))], created=1719298206, model='llama2:13b', object='chat.completion', service_tier=None, system_fingerprint='fp_ollama', usage=CompletionUsage(completion_tokens=47, prompt_tokens=0, total_tokens=47))
    # print(response.choices[0].message.content)













